{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import warnings\n",
    "from collections.abc import Iterable\n",
    "from glob import glob\n",
    "from os.path import dirname, join\n",
    "from statistics import mode\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import torch.optim\n",
    "from IPython.display import HTML, display\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib import rc\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from millify import millify\n",
    "from PIL import Image\n",
    "from pyproj import Transformer\n",
    "from skimage.color import label2rgb\n",
    "from skimage.measure import find_contours, label\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pred2raster import pred2raster\n",
    "from sample_selection import get_components_stats\n",
    "from src.io_operations import (fix_relative_paths, get_image_metadata,\n",
    "                               get_image_pixel_scale, load_args, read_tiff,\n",
    "                               read_yaml)\n",
    "from evaluation import predict_network, evaluate_iteration, evaluate_overlap\n",
    "from utils import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "import os\n",
    "import textwrap\n",
    "from logging import Logger, getLogger\n",
    "from os.path import exists, join\n",
    "from typing import List, Literal, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "from joblib import Parallel, delayed\n",
    "from seaborn import color_palette\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.dataset import DatasetForInference, DatasetFromCoord\n",
    "from src.deepvlab3 import DeepLabv3\n",
    "from src.io_operations import (check_file_extension, convert_tiff_to_npy,\n",
    "                               get_file_extesion, get_image_metadata,\n",
    "                               get_npy_filepath_from_tiff, get_npy_shape,\n",
    "                               load_image, load_norm, read_yaml)\n",
    "from src.logger import create_logger\n",
    "from src.model import build_model, load_weights\n",
    "from src.utils import (add_padding_new, check_folder, extract_patches_coord,\n",
    "                       get_crop_image, get_device, get_pad_width, normalize,\n",
    "                       oversample)\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logger = getLogger(\"__main__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIG_PATH = join(\"figures\")\n",
    "os.makedirs(FIG_PATH, exist_ok=True)\n",
    "\n",
    "# Repo with training data\n",
    "INPUT_PATH = \"../amazon_input_data\"\n",
    "\n",
    "# repo with model outputs\n",
    "VERSION_FOLDER = \"13_amazon_data\"\n",
    "DATA_PATH = join(dirname(os.getcwd()), VERSION_FOLDER)\n",
    "\n",
    "# load args from the version\n",
    "args = load_args(join(DATA_PATH, \"args.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)  # Set the logging level to DEBUG to capture all messages\n",
    "\n",
    "# Create a StreamHandler to output to sys.stdout\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "stream_handler.setLevel(logging.DEBUG)  # Set the handler level to DEBUG\n",
    "\n",
    "# Create a formatter and set it for the handler\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the handler to the logger\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tree = pd.read_csv(join(INPUT_PATH,\"id_trees.csv\"), index_col=\"label_num\")[\"tree_name\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORTHOIMAGE_PATH = args.ortho_image\n",
    "OVERLAPS = args.overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_GT = read_tiff(args.train_segmentation_path)\n",
    "# COMP_TRAIN_GT = label(TRAIN_GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_GT = read_tiff(args.test_segmentation_path)\n",
    "# Data from TRAIN in TEST\n",
    "# TEST_GT = np.where(TRAIN_GT>0, 0, TEST_GT)\n",
    "# COMP_TEST_GT = label(TEST_GT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modificando o Dataset for Inference para considerar uma máscara de segmentação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForInference(Dataset):\n",
    "    def __init__(self,\n",
    "                image_path:str,\n",
    "                crop_size:int,\n",
    "                overlap_rate:float,\n",
    "                mask:np.ndarray,\n",
    "                ) -> None: \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_path = image_path\n",
    "        self.crop_size = crop_size\n",
    "        self.overlap_rate = overlap_rate\n",
    "        \n",
    "        self.image = load_image(image_path)\n",
    "        self.image_shape = self.image.shape\n",
    "        self.mask = mask\n",
    "        self.generate_coords()\n",
    "\n",
    "\n",
    "    def generate_coords(self):\n",
    "        \n",
    "        coords_list = []\n",
    "        \n",
    "        height, width = self.image_shape[-2:]\n",
    "        \n",
    "        self.overlap_size = int(self.crop_size * self.overlap_rate)\n",
    "        self.stride_size = self.crop_size - self.overlap_size\n",
    "\n",
    "        for m in range(0, height-self.overlap_size, self.stride_size):\n",
    "            for n in range(0, width-self.overlap_size, self.stride_size):\n",
    "                                \n",
    "                mask_crop = self.read_window([m, n], self.mask)\n",
    "                \n",
    "                if mask_crop.sum() > 0:\n",
    "                    coords_list.append([m, n])\n",
    "                \n",
    "        \n",
    "        self.coords = np.array(coords_list)\n",
    "\n",
    "\n",
    "    def standardize_image_channels(self):\n",
    "        \n",
    "        self.image = self.image.astype(\"float32\")\n",
    "\n",
    "        normalize(self.image)\n",
    "        \n",
    "\n",
    "    def get_slice_window(self, coord:np.ndarray) -> Tuple[int, int, int, int]:\n",
    "        \"Based on overlap rate and crop size, get the slice to fit the image into original image\"\n",
    "        \n",
    "        row_start = coord[0]\n",
    "        row_end = coord[0] + self.crop_size\n",
    "        \n",
    "        if row_end > self.image_shape[1]:\n",
    "            row_start = self.image_shape[1] - self.crop_size\n",
    "            row_end = self.image_shape[1]\n",
    "        \n",
    "        column_start = coord[1]\n",
    "        column_end = coord[1] + self.crop_size\n",
    "        \n",
    "        if column_end > self.image_shape[2]:\n",
    "            column_start = self.image_shape[2] - self.crop_size\n",
    "            column_end = self.image_shape[2]\n",
    "        \n",
    "        return row_start, row_end, column_start, column_end\n",
    "\n",
    "    def read_window(self, coord:np.ndarray, image:np.ndarray) -> torch.Tensor:\n",
    "        \n",
    "        row_start, row_end, column_start, column_end = self.get_slice_window(coord)\n",
    "        \n",
    "        if len(image.shape) == 2:\n",
    "            image_crop = image[row_start:row_end, column_start:column_end]\n",
    "        \n",
    "        else:\n",
    "            image_crop = image[:, row_start:row_end, column_start:column_end]\n",
    "        \n",
    "        if (image_crop.shape[-1] != self.crop_size) or (image_crop.shape[-2] != self.crop_size):\n",
    "            raise ValueError(f\"There is a bug relationed to the shape {image_crop.shape}\")\n",
    "        \n",
    "        return torch.tensor(image_crop)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get the data from the dataset\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            The index of the data to be loaded\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor, torch.Tensor]\n",
    "            The image crop and the slice to fit the image into original image\n",
    "        \n",
    "        \"\"\"\n",
    "        current_coord = self.coords[idx].copy()\n",
    "        \n",
    "        image = self.read_window(\n",
    "            coord=current_coord,\n",
    "            image=self.image,\n",
    "        )\n",
    "        \n",
    "        row_start, row_end, column_start, column_end = self.get_slice_window(\n",
    "            current_coord,\n",
    "        )\n",
    "        \n",
    "        return image.float(), (row_start, row_end, column_start, column_end)\n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.io_operations import array2raster\n",
    "\n",
    "\n",
    "def evaluate_overlap(overlap:float,\n",
    "                     current_iter_folder:str,\n",
    "                     ortho_image_shape:tuple,\n",
    "                     args\n",
    "                     ):\n",
    "    DEVICE = get_device()\n",
    "\n",
    "    current_model_folder = join(current_iter_folder, args.model_dir)\n",
    "\n",
    "    test_dataset = DatasetForInference(\n",
    "        args.ortho_image,\n",
    "        args.size_crops,\n",
    "        overlap,\n",
    "        mask=(TEST_GT > 0)\n",
    "    )\n",
    "\n",
    "    test_dataset.standardize_image_channels()\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=args.batch_size*2,\n",
    "            num_workers=args.workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "            shuffle=False,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Building data done with {} patches loaded.\".format(test_dataset.coords.shape[0]))\n",
    "    \n",
    "        \n",
    "    model = DeepLabv3(\n",
    "        in_channels = ortho_image_shape[0],\n",
    "        num_classes = args.nb_class, \n",
    "        pretrained = args.is_pretrained, \n",
    "        dropout_rate = args.dropout_rate,\n",
    "        batch_norm = args.batch_norm,\n",
    "        downsampling_factor = args.downsampling_factor,\n",
    "    )\n",
    "\n",
    "\n",
    "    last_checkpoint = join(current_model_folder, args.checkpoint_file)\n",
    "    model = load_weights(model, last_checkpoint)\n",
    "    logger.info(\"Model loaded from {}\".format(last_checkpoint))\n",
    "\n",
    "    # Load model to GPU\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    prob_map, pred_class, depth_map = predict_network(\n",
    "        ortho_image_shape = ortho_image_shape,\n",
    "        dataloader = test_loader,\n",
    "        model = model,\n",
    "        num_classes = args.nb_class\n",
    "    )\n",
    "    del pred_class, depth_map, test_dataset, test_loader\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    logger.info(f\"Saving prediction outputs..\")\n",
    "    \n",
    "    return prob_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_iteration(current_iter_folder:str, args:dict):\n",
    "\n",
    "    ortho_image_metadata = get_image_metadata(args.ortho_image)\n",
    "    \n",
    "    ortho_image_shape = (ortho_image_metadata[\"count\"], ortho_image_metadata[\"height\"], ortho_image_metadata[\"width\"])\n",
    "    \n",
    "    logger.info(\"============ Initialized Evaluation ============\")\n",
    "    path_to_save = join(current_iter_folder, \"prob_map_test.tif\")\n",
    "    \n",
    "    if exists(path_to_save):\n",
    "        logger.info(\"Prediction already done. Skipping...\")\n",
    "        return\n",
    "    \n",
    "    for num, overlap in enumerate(args.overlap):\n",
    "        \n",
    "        prediction_path = join(current_iter_folder, f'prediction_{overlap}.npz')\n",
    "        \n",
    "        is_prediction_overlap_done = exists(prediction_path)\n",
    "\n",
    "        if is_prediction_overlap_done:\n",
    "        \n",
    "            logger.info(f\"Overlap {overlap} is already done. Skipping...\")\n",
    "\n",
    "            continue\n",
    "        \n",
    "        logger.info(f\"Overlap {overlap} is not done. Starting...\")\n",
    "        if num == 0:\n",
    "            prob_map = evaluate_overlap(\n",
    "                overlap, \n",
    "                current_iter_folder, \n",
    "                ortho_image_shape,\n",
    "                args=args)\n",
    "        \n",
    "        else:\n",
    "            prob_map += evaluate_overlap(\n",
    "                overlap, \n",
    "                current_iter_folder, \n",
    "                ortho_image_shape,\n",
    "                args=args)\n",
    "    \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    prob_map = prob_map / len(args.overlap)\n",
    "    \n",
    "    array2raster(\n",
    "        path_to_save=path_to_save,\n",
    "        array=prob_map,\n",
    "        metadata=ortho_image_metadata,\n",
    "    )\n",
    "    print(\"Saved to \", path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iter_folders(output_folder):\n",
    "    # load data from all iterations\n",
    "    iter_folders = os.listdir(output_folder)\n",
    "\n",
    "    iter_folders = [join(output_folder, folder) for folder in iter_folders if folder.startswith(\"iter_\")]\n",
    "\n",
    "    iter_folders.sort()\n",
    "    iter_folders = iter_folders[1:-1].copy()\n",
    "    \n",
    "    return iter_folders.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_folders = get_iter_folders(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-27 22:23:50,630 - root - INFO - ============ Initialized Evaluation ============\n",
      "2024-10-27 22:23:50,630 - root - INFO - ============ Initialized Evaluation ============\n",
      "2024-10-27 22:23:50,630 - root - INFO - ============ Initialized Evaluation ============\n",
      "2024-10-27 22:23:50,631 - root - INFO - Overlap 0.1 is not done. Starting...\n",
      "2024-10-27 22:23:50,631 - root - INFO - Overlap 0.1 is not done. Starting...\n",
      "2024-10-27 22:23:50,631 - root - INFO - Overlap 0.1 is not done. Starting...\n",
      "2024-10-27 22:24:17,371 - root - INFO - Building data done with 648 patches loaded.\n",
      "2024-10-27 22:24:17,371 - root - INFO - Building data done with 648 patches loaded.\n",
      "2024-10-27 22:24:17,371 - root - INFO - Building data done with 648 patches loaded.\n",
      "2024-10-27 22:24:17,815 - __main__ - INFO - Load pretrained model with msg: <All keys matched successfully>\n",
      "2024-10-27 22:24:17,815 - __main__ - INFO - Load pretrained model with msg: <All keys matched successfully>\n",
      "2024-10-27 22:24:17,815 - __main__ - INFO - Load pretrained model with msg: <All keys matched successfully>\n",
      "2024-10-27 22:24:17,817 - root - INFO - Model loaded from /home/luiz.luz/multi-task-fcn/13_amazon_data/iter_001/exp_deeplab_v4/checkpoint.pth.tar\n",
      "2024-10-27 22:24:17,817 - root - INFO - Model loaded from /home/luiz.luz/multi-task-fcn/13_amazon_data/iter_001/exp_deeplab_v4/checkpoint.pth.tar\n",
      "2024-10-27 22:24:17,817 - root - INFO - Model loaded from /home/luiz.luz/multi-task-fcn/13_amazon_data/iter_001/exp_deeplab_v4/checkpoint.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [01:10<00:00,  3.34s/it]\n"
     ]
    }
   ],
   "source": [
    "for iter_folder in iter_folders:\n",
    "    \n",
    "    evaluate_iteration(iter_folder, args)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
