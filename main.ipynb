{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "from logging import getLogger\n",
    "from logging import Logger\n",
    "\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from generate_distance_map import generate_train_test_map, is_generated\n",
    "\n",
    "from src.logger import create_logger\n",
    "from src.metrics import evaluate_metrics\n",
    "from src.model import define_loader, build_model, load_weights, train, save_checkpoint\n",
    "from src.multicropdataset import DatasetFromCoord\n",
    "from src.resnet import ResUnet\n",
    "from src.utils import (\n",
    "    initialize_exp,\n",
    "    restart_from_checkpoint,\n",
    "    fix_random_seeds,\n",
    "    AverageMeter,\n",
    "    read_tiff,\n",
    "    load_norm,\n",
    "    check_folder,\n",
    "    plot_figures,\n",
    "    print_sucess,\n",
    "    oversamp,\n",
    "    read_yaml\n",
    ")\n",
    "from evaluation import evaluate_iteration\n",
    "from pred2raster import pred2raster\n",
    "def first_output_exists(data_path, test_itc, overlap):\n",
    "    output_path = os.path.join(data_path, \"iter_1\", \"raster_prediction\")\n",
    "    \n",
    "    depth_path = os.path.join(output_path, f\"depth_itc{test_itc}_{np.sum(overlap)}.TIF\")\n",
    "    is_depth_done = os.path.isfile(depth_path)\n",
    "\n",
    "    prob_path = os.path.join(output_path, f\"join_prob_itc{test_itc}_{np.sum(overlap)}.TIF\")\n",
    "    is_prob_done = os.path.isfile(prob_path)\n",
    "\n",
    "    class_path = os.path.join(output_path, f\"join_class_itc{test_itc}_{np.sum(overlap)}.TIF\")\n",
    "    is_class_done = os.path.isfile(class_path)\n",
    "\n",
    "    return is_depth_done and is_prob_done and is_class_done\n",
    "\n",
    "# def is_first_iter(data_path, test_itc, overlap):\n",
    "#     \"\"\"Check if it is the first iteration of the training process\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     bool\n",
    "#         True if it is the first iteration, False otherwise\n",
    "#     \"\"\"\n",
    "#     return \"iter_\" not in os.listdir(data_path) and not first_output_exists(data_path, test_itc, overlap)\n",
    "\n",
    "def get_current_iter_folder(data_path, test_itc, overlap):\n",
    "    \"\"\"Get the current iteration folder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : str\n",
    "        Path to the data folder \n",
    "    test_itc : bool\n",
    "        Parameter used for create the output file name\n",
    "    overlap : float\n",
    "        Parameter used for create the output file name\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The path to the current iteration folder\n",
    "    \"\"\"\n",
    "\n",
    "    folders = pd.Series(os.listdir(data_path))\n",
    "    iter_folders = folders[folders.str.contains(\"iter_\")].sort_values(ascending=False).to_list()\n",
    "\n",
    "    for idx, iter_folder_name in enumerate(iter_folders[1:]):\n",
    "        iter_path = os.path.join(data_path, iter_folder_name)\n",
    "\n",
    "        prediction_path = os.path.join(iter_path, \"raster_prediction\")\n",
    "        is_folder_generated = os.path.exists(prediction_path)\n",
    "        is_depth_done = os.path.isfile(os.path.join(prediction_path, f\"depth_itc{test_itc}_{np.sum(overlap)}.TIF\"))\n",
    "        is_class_done = os.path.isfile(os.path.join(prediction_path, f\"join_class_itc{test_itc}_{np.sum(overlap)}.TIF\"))\n",
    "        is_prob_done = os.path.isfile(os.path.join(prediction_path, f\"join_prob_itc{test_itc}_{np.sum(overlap)}.TIF\"))\n",
    "        \n",
    "        if is_folder_generated and is_depth_done and is_class_done and is_prob_done:\n",
    "            current_folder = iter_folders[idx]\n",
    "            current_path = os.path.join(data_path, current_folder)\n",
    "            return current_path\n",
    "    \n",
    "    iter_1_path = os.path.join(data_path, \"iter_1\")\n",
    "    if os.path.exists(iter_1_path):\n",
    "        return iter_1_path\n",
    "    else:\n",
    "        raise FileExistsError(\"No finished iteration folder found. Try execute generate_distance_map first.\")\n",
    "    \n",
    "# # deprecated\n",
    "# def get_iter_folder(data_path, test_itc, overlap):\n",
    "#     if is_first_iter(data_path, test_itc, overlap):\n",
    "#         print(\"First iteration\")\n",
    "#         return os.path.join(data_path, \"iter_1\")\n",
    "#     else:\n",
    "#         paths = pd.Series(os.listdir(data_path))\n",
    "#         paths = paths[paths.str.contains(\"iter\")].sort_values()\n",
    "#         last_iter_folder = paths.iloc[-1]\n",
    "#         # is_iter_finished = is_generated(last_iter_folder,os.path.join(data_path, last_iter_folder, \"before_iter\", \"train_distance_map.tif\"))\n",
    "        \n",
    "#         # return os.path.join(data_path, last_iter_folder)\n",
    "\n",
    "\n",
    "def read_last_segmentation(current_iter_folder, data_path, train_segmentation_file, test_itc, overlap):\n",
    "    current_iter = int(current_iter_folder.split(\"_\")[-1])\n",
    "    if current_iter==1:\n",
    "        image_path = os.path.join(data_path, \"segmentation\", train_segmentation_file)\n",
    "        \n",
    "    else:\n",
    "        image_path = os.path.join(data_path, \"iter_\"+str(current_iter-1), \"raster_prediction\", f\"join_class_itc{test_itc}_{np.sum(overlap)}.TIF\")\n",
    "\n",
    "    image = read_tiff(image_path)\n",
    "    return image\n",
    "\n",
    "    \n",
    "\n",
    "def read_last_distance_map(current_iter_folder, data_path, test_itc, overlap):\n",
    "    current_iter = int(current_iter_folder.split(\"_\")[-1])\n",
    "    if current_iter == 1 :\n",
    "        image_path = os.path.join(data_path, \"before_iter\", \"train_distance_map.tif\")\n",
    "        \n",
    "    else:\n",
    "        image_path = os.path.join(data_path, \"iter_\"+str(current_iter-1), \"raster_prediction\", f\"depth_itc{test_itc}_{np.sum(overlap)}.TIF\")\n",
    "        \n",
    "    image = read_tiff(image_path)\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_learning_rate_schedule(train_loader: torch.utils.data.DataLoader,base_lr:float,final_lr:float, epochs:int, warmup_epochs:int, start_warmup:float)->np.array:\n",
    "    \"\"\"Get the learning rate schedule using cosine annealing with warmup\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_loader : torch.utils.data.DataLoader\n",
    "        Model train loader\n",
    "    base_lr : float\n",
    "        base learning rate\n",
    "    final_lr : float\n",
    "        final learning rate\n",
    "    epochs : int\n",
    "        number of total epochs to run\n",
    "    warmup_epochs : int\n",
    "        number of warmup epochs\n",
    "    start_warmup : float\n",
    "        initial warmup learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        learning rate schedule\n",
    "    \"\"\"\n",
    "    # define learning rate schedule\n",
    "    warmup_lr_schedule = np.linspace(start_warmup, base_lr, len(train_loader) * warmup_epochs)\n",
    "    # iteration numbers\n",
    "    iters = np.arange(len(train_loader) * (epochs - warmup_epochs))\n",
    "\n",
    "    cosine_lr_schedule = []\n",
    "    for t in iters:\n",
    "        lr = final_lr + 0.5 * (base_lr - final_lr) * (1 + math.cos(math.pi * t / (len(train_loader) * (epochs - warmup_epochs))))\n",
    "        cosine_lr_schedule.append(lr)\n",
    "    cosine_lr_schedule = np.array(cosine_lr_schedule)\n",
    "\n",
    "    lr_schedule = np.concatenate((warmup_lr_schedule, cosine_lr_schedule))\n",
    "    return lr_schedule\n",
    "\n",
    "\n",
    "def train_epochs(last_checkpoint, start_epoch, num_epochs, best_val, train_loader, model, optimizer, lr_schedule, rank, count_early, patience:int=20):\n",
    "    # Create figures folder to save training figures every epoch\n",
    "    figures_path = os.path.join(os.path.dirname(last_checkpoint), 'figures')\n",
    "    check_folder(figures_path)\n",
    "\n",
    "    for epoch in tqdm(range(start_epoch, num_epochs)):\n",
    "\n",
    "        if rank == 0:\n",
    "            if count_early == patience:\n",
    "                logger.info(\"============ Early Stop at epoch %i ... ============\" % epoch)\n",
    "                break\n",
    "        \n",
    "        np.random.shuffle(train_loader.dataset.coord)\n",
    "\n",
    "        # train the network for one epoch\n",
    "        logger.info(\"============ Starting epoch %i ... ============\" % epoch)\n",
    "\n",
    "        # train the network\n",
    "        scores_tr = train(train_loader, model, optimizer, epoch, lr_schedule, figures_path, logger)\n",
    "        \n",
    "        training_stats.update(scores_tr)\n",
    "        \n",
    "        print_sucess(\"scores_tr: {}\".format(scores_tr[1]))\n",
    "\n",
    "        is_best = scores_tr[1] <= best_val\n",
    "\n",
    "        # save checkpoints\n",
    "        if rank == 0:\n",
    "            if is_best:\n",
    "                logger.info(\"============ Saving best models at epoch %i ... ============\" % epoch)\n",
    "                best_val = scores_tr[1]\n",
    "                save_checkpoint(last_checkpoint, model, optimizer, epoch, best_val)\n",
    "            else:\n",
    "                count_early+=1\n",
    "                \n",
    "            \n",
    "    print_sucess(\"Training done !\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def train_iteration(current_iter_folder, args):\n",
    "    ######### Define Loader ############\n",
    "    raster_train = read_last_segmentation(current_iter_folder, args.data_path, args.train_segmentation_file, args.test_itc, args.overlap)\n",
    "    depth_img = read_last_distance_map(current_iter_folder, args.data_path, args.test_itc, args.overlap)\n",
    "\n",
    "    image, coords_train, raster_train, labs_coords_train = define_loader(args.ortho_image, raster_train, args.size_crops)    \n",
    "\n",
    "    ######## do oversampling in minor classes\n",
    "    coords_train = oversamp(coords_train, labs_coords_train,under=False)\n",
    "\n",
    "    if args.samples > coords_train.shape[0]:\n",
    "        args.samples = None\n",
    "\n",
    "    # build data for training\n",
    "    train_dataset = DatasetFromCoord(\n",
    "        image,\n",
    "        raster_train,\n",
    "        depth_img,\n",
    "        coords_train,\n",
    "        args.size_crops,\n",
    "        args.samples,\n",
    "        augment = args.augment\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    logger.info(\"Building data done with {} images loaded.\".format(len(train_loader)))\n",
    "\n",
    "    num_classes = len(np.unique(labs_coords_train))\n",
    "\n",
    "    model = build_model(image.shape, num_classes,  args.arch, args.filters, args.is_pretrained)\n",
    "\n",
    "    last_checkpoint = os.path.join(current_model_folder, args.checkpoint_file)\n",
    "    model = load_weights(model, last_checkpoint, logger)\n",
    "\n",
    "    # Load model to GPU\n",
    "    model = model.cuda()\n",
    "\n",
    "\n",
    "    if args.rank == 0:\n",
    "        logger.info(model)\n",
    "    logger.info(\"Building model done.\")\n",
    "\n",
    "    # build optimizer\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=args.base_lr,\n",
    "        momentum=0.9,\n",
    "        weight_decay=args.weight_decay\n",
    "    )\n",
    "\n",
    "    lr_schedule = get_learning_rate_schedule(\n",
    "        train_loader, \n",
    "        args.base_lr, \n",
    "        args.final_lr, \n",
    "        args.epochs, \n",
    "        args.warmup_epochs, \n",
    "        args.start_warmup\n",
    "    )\n",
    "    logger.info(\"Building optimizer done.\")\n",
    "\n",
    "\n",
    "    to_restore = {\"epoch\": 0, \"best_acc\":(0.), \"count_early\": 0,\"is_iter_finished\":False}\n",
    "    restart_from_checkpoint(\n",
    "        last_checkpoint,\n",
    "        run_variables=to_restore,\n",
    "        state_dict=model,\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "    start_epoch = to_restore[\"epoch\"]\n",
    "    best_val = to_restore[\"best_acc\"]\n",
    "    count_early = to_restore[\"count_early\"]\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "    # if to_restore[\"is_iter_finished\"]:\n",
    "    #     return\n",
    "    \n",
    "    train_epochs(last_checkpoint, start_epoch, args.epochs, best_val , train_loader, model, optimizer, lr_schedule, args.rank, count_early)\n",
    "\n",
    "    # load models weights again to change status\n",
    "    model = load_weights(model, last_checkpoint, logger)\n",
    "\n",
    "    to_restore = {\"epoch\": 0, \"best_acc\":(0.), \"count_early\": 0, \"is_iter_finished\":False}\n",
    "    restart_from_checkpoint(\n",
    "        last_checkpoint,\n",
    "        run_variables=to_restore,\n",
    "        state_dict=model,\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "    \n",
    "    # save here next iteration\n",
    "    save_checkpoint(last_checkpoint, model, optimizer, to_restore[\"epoch\"], to_restore[\"best_acc\"],to_restore[\"count_early\"] ,is_iter_finished=True)\n",
    "    \n",
    "    next_iter_folder = os.path.join(args.data_path, args.model_dir, \"iter_\"+str(current_iter+1))\n",
    "    check_folder(next_iter_folder)\n",
    "    save_checkpoint(os.path.join(next_iter_folder, args.checkpoint_file), model, optimizer, 0, 100, 0 ,is_iter_finished=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############\n",
    "### SETUP ###\n",
    "#############\n",
    "\n",
    "logger = getLogger('swav_model')\n",
    "args = read_yaml(\"args.yaml\")\n",
    "\n",
    "\n",
    "##### LOOP #####\n",
    "\n",
    "# Generate distance map if necessary\n",
    "generate_train_test_map()\n",
    "# Set random seed\n",
    "fix_random_seeds(args.seed[0])\n",
    "\n",
    "for i in range(0,20):\n",
    "    # get current iteration folder\n",
    "    current_iter_folder = get_current_iter_folder(args.data_path, args.test_itc, args.overlap)\n",
    "    current_iter = int(current_iter_folder.split(\"_\")[-1])\n",
    "\n",
    "    # Create model folder for checkpoint\n",
    "    current_model_folder = os.path.join(current_iter_folder, args.model_dir)\n",
    "    check_folder(current_model_folder)\n",
    "\n",
    "    logger, training_stats = initialize_exp(args, \"epoch\", \"loss\")\n",
    "\n",
    "    train_iteration(current_iter_folder, args)\n",
    "\n",
    "    evaluate_iteration(current_iter_folder, args)\n",
    "\n",
    "    pred2raster(current_iter_folder, args)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
